{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOW+Pwu9ZIMTaeQhyF70g/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heewonLEE2/Data-Ai-Colab/blob/main/CIFAR10_%EC%B4%88%EA%B8%B0%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# CIFAR-10 Colab Starter: Transfer Learning / From-Scratch\n",
        "# =========================================================\n",
        "!nvidia-smi -L || True  # GPU 확인(없어도 무시)\n",
        "\n",
        "import os, math, random, time\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "ndJO3FmB2Imv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "DATASET = 'CIFAR10'   # 'CIFAR10' or 'CIFAR100'\n",
        "NUM_CLASSES = 10      # 100 if CIFAR100\n",
        "MODEL_NAME = 'resnet18'  # 'resnet18' or 'cnn'\n",
        "FULL_FINETUNE = True  # ResNet: 전체 미세조정 여부 (False면 헤드만 학습)\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30\n",
        "LR = 3e-4             # AdamW 기본 러닝레이트 (ResNet 기준)\n",
        "WD = 0.05             # Weight Decay\n",
        "LABEL_SMOOTH = 0.1\n",
        "WARMUP_EPOCHS = 3\n",
        "PATIENCE = 7          # EarlyStopping patience\n",
        "NUM_WORKERS = os.cpu_count() if os.cpu_count() else 2\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cudnn.benchmark = True if DEVICE=='cuda' else False"
      ],
      "metadata": {
        "id": "_NFTNXAE7uxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(SEED)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "        self.should_stop = False\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        if self.best is None or val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup, num_training):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup:\n",
        "            return float(current_step) / float(max(1, num_warmup))\n",
        "        progress = float(current_step - num_warmup) / float(max(1, num_training - num_warmup))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        pred = output.argmax(dim=1)\n",
        "        return (pred == target).float().mean().item()\n",
        "\n",
        "def save_checkpoint(model, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "T32hn0LA9gzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Data: transforms & loaders\n",
        "# ---------------------------\n",
        "if MODEL_NAME == 'resnet18':\n",
        "    # ImageNet 통계 (ResNet 권장)\n",
        "    mean = (0.485, 0.456, 0.406); std = (0.229, 0.224, 0.225)\n",
        "    train_tf = T.Compose([\n",
        "        T.Resize(224),\n",
        "        AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = T.Compose([\n",
        "        T.Resize(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "else:\n",
        "    # CIFAR 통계 (작은 CNN 권장)\n",
        "    mean = (0.4914, 0.4822, 0.4465); std = (0.2470, 0.2435, 0.2616)\n",
        "    train_tf = T.Compose([\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ColorJitter(0.1,0.1,0.1,0.1),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "DatasetClass = getattr(torchvision.datasets, DATASET)\n",
        "trainset = DatasetClass(root='./data', train=True, download=True, transform=train_tf)\n",
        "testset  = DatasetClass(root='./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "# train/val split\n",
        "val_ratio = 0.1\n",
        "num_train = len(trainset)\n",
        "indices = np.arange(num_train)\n",
        "np.random.shuffle(indices)\n",
        "split = int(num_train * (1 - val_ratio))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_subset = torch.utils.data.Subset(trainset, train_idx)\n",
        "val_subset   = torch.utils.data.Subset(trainset, val_idx)\n",
        "\n",
        "pin = True if DEVICE=='cuda' else False\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin)\n",
        "val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin)\n",
        "test_loader  = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin)"
      ],
      "metadata": {
        "id": "dGb7gGkP-ab4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128,128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.2),\n",
        "\n",
        "            nn.Conv2d(128,256, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256,256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "if MODEL_NAME == 'resnet18':\n",
        "    weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
        "    model = models.resnet18(weights=weights)\n",
        "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "    if not FULL_FINETUNE:\n",
        "        for p in model.parameters(): p.requires_grad = False\n",
        "        for p in model.fc.parameters(): p.requires_grad = True\n",
        "else:\n",
        "    model = SmallCNN(NUM_CLASSES)\n",
        "\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "0-TpM4EA_ZYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Optimizer / Loss / Scheduler\n",
        "# ---------------------------\n",
        "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WD)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "scaler = GradScaler(enabled=(DEVICE=='cuda'))\n",
        "\n",
        "# 전체 스텝 수 기준 스케줄러 (warmup + cosine)\n",
        "total_steps = EPOCHS * math.ceil(len(train_loader))\n",
        "warmup_steps = WARMUP_EPOCHS * math.ceil(len(train_loader))\n",
        "scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)"
      ],
      "metadata": {
        "id": "CJyu4WjDApxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Train / Eval\n",
        "# ---------------------------\n",
        "best_val = float('inf')\n",
        "early = EarlyStopping(patience=PATIENCE)\n",
        "CKPT = '/content/best_cifar.pt'\n",
        "\n",
        "def run_one_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    running_loss, running_acc = 0.0, 0.0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(DEVICE=='cuda')):\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_acc  += accuracy(logits, labels) * images.size(0)\n",
        "\n",
        "    n = len(loader.dataset)\n",
        "    return running_loss / n, running_acc / n\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = run_one_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc     = run_one_epoch(val_loader, train=False)\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        save_checkpoint(model, CKPT)\n",
        "\n",
        "    early.step(val_loss)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | {dt:.1f}s\")\n",
        "    if early.should_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ],
      "metadata": {
        "id": "QBaN6YU8CNeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Test with best checkpoint\n",
        "# ---------------------------\n",
        "model.load_state_dict(torch.load(CKPT, map_location=DEVICE))\n",
        "model.eval()\n",
        "test_loss, test_acc = run_one_epoch(test_loader, train=False)\n",
        "print(f\"Test: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix (간단 버전)\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def confusion_matrix(model, loader, num_classes=NUM_CLASSES):\n",
        "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
        "    for images, labels in loader:\n",
        "        images = images.to(DEVICE); labels = labels.to(DEVICE)\n",
        "        logits = model(images); preds = logits.argmax(1)\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "    return cm.cpu().numpy()\n",
        "\n",
        "cm = confusion_matrix(model, test_loader, NUM_CLASSES)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.colorbar(); plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "XJQYx8-iGbyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❗ 위에 학습속도가 너무 느려 1 epoch 돌리는데 많은 시간을 소요해 성능을 포기하고 빠르게 도는 방식으로 다시 리팩토링\n",
        "- 아래의 방식으로 돌리니 학습속도가 5~8배는 넘게 빨라짐\n",
        "- 성능의 차이도 알면 좋겠다."
      ],
      "metadata": {
        "id": "ml7JvMXNF_fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Fast CIFAR-10 Trainer (Colab/T4 최적화판)\n",
        "# - 32×32 입력 유지 (연산량 최소화)\n",
        "# - SmallCNN 기본 (아주 빠름) / CIFAR-stem ResNet18 옵션\n",
        "# - AMP + 간단한 증강 + 안정적인 DataLoader 설정\n",
        "# =========================================================\n",
        "!nvidia-smi -L || True\n",
        "\n",
        "import os, math, time, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch import amp  # <-- 최신 권장 autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# ---------------------------\n",
        "# Config (속도 우선 프로파일)\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "DATASET = 'CIFAR10'       # 'CIFAR10' or 'CIFAR100'\n",
        "NUM_CLASSES = 10          # CIFAR100이면 100으로 변경\n",
        "MODEL = 'smallcnn'        # 'smallcnn' (기본, 매우 빠름) or 'cifar_resnet18'\n",
        "EPOCHS = 15               # 빠른 실험 기본\n",
        "BATCH_SIZE = 256          # T4에서 보통 256~512까지 시도 가능(메모리 보고 조정)\n",
        "LR = 1e-3                 # AdamW 기본 러닝레이트\n",
        "WD = 0.01                 # weight decay (살짝만)\n",
        "LABEL_SMOOTH = 0.0        # 속도/안정 위해 0으로 (원하면 0.1)\n",
        "WARMUP_EPOCHS = 1         # 짧은 워밍업\n",
        "PATIENCE = 5              # early stopping\n",
        "NUM_WORKERS = 2           # Colab은 2~4가 안전. 멈추면 0으로 내리기\n",
        "PIN = True\n",
        "PERSISTENT = False        # Colab 프리징 방지\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.backends.cudnn.benchmark = (DEVICE=='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCUvmyaYGKND",
        "outputId": "f3d02afe-bc92-4d37-a09e-5ed3365578f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1f6874e9-b9d3-ca2e-b467-be622eebe3ba)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE, min_delta=0.0):\n",
        "        self.patience = patience; self.min_delta = min_delta\n",
        "        self.best = None; self.counter = 0; self.should_stop = False\n",
        "    def step(self, val_loss):\n",
        "        if self.best is None or val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss; self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup, num_training):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup:\n",
        "            return float(current_step) / float(max(1, num_warmup))\n",
        "        progress = float(current_step - num_warmup) / float(max(1, num_training - num_warmup))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target):\n",
        "    return (output.argmax(1) == target).float().mean().item()\n",
        "\n",
        "def save_ckpt(model, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "SGQKW0_EHEk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Data (32×32 유지, 가벼운 증강)\n",
        "# ---------------------------\n",
        "if DATASET == 'CIFAR100':\n",
        "    DatasetClass = torchvision.datasets.CIFAR100\n",
        "    NUM_CLASSES = 100\n",
        "else:\n",
        "    DatasetClass = torchvision.datasets.CIFAR10\n",
        "    NUM_CLASSES = 10\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465); std = (0.2470, 0.2435, 0.2616)\n",
        "train_tf = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "test_tf = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = DatasetClass(root='./data', train=True,  download=True, transform=train_tf)\n",
        "testset  = DatasetClass(root='./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "# train/val split (9:1)\n",
        "idx = np.arange(len(trainset)); np.random.shuffle(idx)\n",
        "split = int(len(idx) * 0.9)\n",
        "train_idx, val_idx = idx[:split], idx[split:]\n",
        "train_subset = torch.utils.data.Subset(trainset, train_idx)\n",
        "val_subset   = torch.utils.data.Subset(trainset, val_idx)\n",
        "\n",
        "# DataLoader (prefetch_factor는 workers>0일 때만 설정)\n",
        "dl_kwargs = dict(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=PERSISTENT)\n",
        "if NUM_WORKERS and NUM_WORKERS > 0:\n",
        "    dl_kwargs.update(prefetch_factor=2)\n",
        "\n",
        "train_loader = DataLoader(train_subset, shuffle=True, **dl_kwargs)\n",
        "val_loader   = DataLoader(val_subset, shuffle=False, **dl_kwargs)\n",
        "test_loader  = DataLoader(testset, shuffle=False, **dl_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Yrq28oHYET",
        "outputId": "257decf6-0672-45f4-f684-7f4238cd5291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:07<00:00, 24.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Models (속도 우선)\n",
        "# ---------------------------\n",
        "class SmallCNN(nn.Module):\n",
        "    # 채널 수를 절제해 아주 빠르게\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.05),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
        "\n",
        "            nn.Conv2d(64,128,3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128,128,3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 128), nn.ReLU(inplace=True), nn.Dropout(0.25),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x): return self.classifier(self.features(x))\n",
        "\n",
        "def build_cifar_resnet18(num_classes):\n",
        "    from torchvision import models\n",
        "    m = models.resnet18(weights=None)          # 전이학습보다 from-scratch가 훨씬 가벼움\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # CIFAR stem\n",
        "    m.maxpool = nn.Identity()\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "if MODEL == 'cifar_resnet18':\n",
        "    model = build_cifar_resnet18(NUM_CLASSES)\n",
        "else:\n",
        "    model = SmallCNN(NUM_CLASSES)\n",
        "\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "1Y3QtslFHktV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Optim / Loss / Scheduler\n",
        "# ---------------------------\n",
        "opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "scaler = GradScaler(enabled=(DEVICE=='cuda'))\n",
        "\n",
        "total_steps = EPOCHS * len(train_loader)\n",
        "warmup_steps = max(1, WARMUP_EPOCHS * len(train_loader))\n",
        "scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8QV0Jh2H1ux",
        "outputId": "21038103-5ec0-4549-9d0f-681d1a05ec42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3065940662.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(DEVICE=='cuda'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Train / Eval\n",
        "# ---------------------------\n",
        "CKPT = '/content/best_fast_cifar.pt'\n",
        "best_val = float('inf')\n",
        "early = EarlyStopping(patience=PATIENCE)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for x,y in loader:\n",
        "        x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with amp.autocast(device_type='cuda', enabled=(DEVICE=='cuda')):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
        "\n",
        "    if va_loss < best_val:\n",
        "        best_val = va_loss\n",
        "        save_ckpt(model, CKPT)\n",
        "\n",
        "    early.step(va_loss)\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
        "          f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
        "          f\"val_loss={va_loss:.4f} acc={va_acc:.4f} | \"\n",
        "          f\"{time.time()-t0:.1f}s\")\n",
        "    if early.should_stop:\n",
        "        print(\"Early stopping.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxxLH8JDH9XE",
        "outputId": "07e51624-ad42-40c3-9fb5-7428344682d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/15] train_loss=1.7657 acc=0.3346 | val_loss=1.5909 acc=0.4300 | 43.1s\n",
            "[02/15] train_loss=1.1862 acc=0.5688 | val_loss=1.0401 acc=0.6346 | 18.6s\n",
            "[03/15] train_loss=0.9660 acc=0.6556 | val_loss=1.1108 acc=0.6186 | 22.0s\n",
            "[04/15] train_loss=0.8357 acc=0.7049 | val_loss=0.8069 acc=0.7160 | 21.1s\n",
            "[05/15] train_loss=0.7400 acc=0.7400 | val_loss=0.7580 acc=0.7322 | 20.2s\n",
            "[06/15] train_loss=0.6695 acc=0.7657 | val_loss=0.6877 acc=0.7612 | 19.5s\n",
            "[07/15] train_loss=0.6178 acc=0.7879 | val_loss=0.6687 acc=0.7736 | 18.5s\n",
            "[08/15] train_loss=0.5668 acc=0.8037 | val_loss=0.5681 acc=0.8040 | 19.3s\n",
            "[09/15] train_loss=0.5336 acc=0.8156 | val_loss=0.5380 acc=0.8138 | 19.0s\n",
            "[10/15] train_loss=0.5047 acc=0.8252 | val_loss=0.5371 acc=0.8180 | 19.0s\n",
            "[11/15] train_loss=0.4775 acc=0.8341 | val_loss=0.5195 acc=0.8224 | 19.1s\n",
            "[12/15] train_loss=0.4559 acc=0.8419 | val_loss=0.4999 acc=0.8310 | 18.5s\n",
            "[13/15] train_loss=0.4390 acc=0.8491 | val_loss=0.4875 acc=0.8342 | 19.5s\n",
            "[14/15] train_loss=0.4320 acc=0.8522 | val_loss=0.4857 acc=0.8352 | 18.5s\n",
            "[15/15] train_loss=0.4277 acc=0.8498 | val_loss=0.4843 acc=0.8324 | 19.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Test with the best checkpoint\n",
        "# ---------------------------\n",
        "model.load_state_dict(torch.load(CKPT, map_location=DEVICE))\n",
        "model.eval()\n",
        "te_loss, te_acc = run_epoch(test_loader, train=False)\n",
        "print(f\"Test: loss={te_loss:.4f}, acc={te_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dTMO8MIEQn",
        "outputId": "97fff983-cd3a-460b-bae3-f8c27ff05477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: loss=0.4830, acc=0.8359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ 더 좋은 성능을 내보기 위해 몇개의 하이퍼 파라미터를 변경\n",
        "- 모델: SmallCNN → CIFAR-stem ResNet18 (7×7/stride2/maxpool 제거, 32×32 최적화).\n",
        "- 증강: RandAugment(n=1) + Cutout(16) 추가(가벼우면서 일반화↑).\n",
        "- 정규화 트릭: label_smoothing=0.1, weight_decay=0.02.\n",
        "- 학습 스케줄: 30epoch, Cosine + Warmup(2epoch), EarlyStopping(patience=7).\n",
        "- 속도 트릭: AMP, 큰 배치(384), channels_last, 안정적 DataLoader 설정."
      ],
      "metadata": {
        "id": "hYj-iUzfJV4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# CIFAR-10 (Improved Fast Baseline)\n",
        "# - 32×32 입력 유지 + CIFAR-stem ResNet18 (from scratch)\n",
        "# - RandAugment(n=1) + Cutout(16)\n",
        "# - Label Smoothing 0.1, WD 0.02\n",
        "# - AMP, 큰 배치(384), 30 epoch, 안정적 DataLoader\n",
        "# =========================================================\n",
        "!nvidia-smi -L || True\n",
        "\n",
        "import os, math, time, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch import amp\n",
        "from torch.cuda.amp import GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "DATASET = 'CIFAR10'     # 'CIFAR10' or 'CIFAR100'\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 384        # 메모리 여유면 512도 시도 가능\n",
        "LR = 1.5e-3             # 배치↑에 맞춰 살짝 상향(256→384, 약 1.5배)\n",
        "WD = 0.02\n",
        "LABEL_SMOOTH = 0.10\n",
        "WARMUP_EPOCHS = 2\n",
        "PATIENCE = 7\n",
        "NUM_WORKERS = 2         # 멈추면 0으로\n",
        "PIN = True\n",
        "PERSISTENT = False\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.backends.cudnn.benchmark = (DEVICE=='cuda')\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "# ---------------------------\n",
        "# Small util\n",
        "# ---------------------------\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE, min_delta=0.0):\n",
        "        self.patience=patience; self.min_delta=min_delta; self.best=None; self.counter=0; self.should_stop=False\n",
        "    def step(self, v):\n",
        "        if self.best is None or v < self.best - self.min_delta:\n",
        "            self.best = v; self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience: self.should_stop = True\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup, num_training):\n",
        "    def lr_lambda(step):\n",
        "        if step < num_warmup: return float(step) / float(max(1, num_warmup))\n",
        "        prog = float(step - num_warmup) / float(max(1, num_training - num_warmup))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * prog))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(logits, y): return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "def save_ckpt(model, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "# ---------------------------\n",
        "# Augmentations (RandAugment + Cutout)\n",
        "# ---------------------------\n",
        "class Cutout(object):\n",
        "    def __init__(self, size=16):\n",
        "        self.size = size\n",
        "    def __call__(self, img):\n",
        "        # img: Tensor[C,H,W], 값 [0,1]\n",
        "        if not isinstance(img, torch.Tensor): return img\n",
        "        c, h, w = img.shape\n",
        "        cx = random.randint(0, w-1); cy = random.randint(0, h-1)\n",
        "        half = self.size // 2\n",
        "        x1, y1 = max(0, cx - half), max(0, cy - half)\n",
        "        x2, y2 = min(w, cx + half), min(h, cy + half)\n",
        "        img[:, y1:y2, x1:x2] = 0.0\n",
        "        return img\n",
        "\n",
        "# ---------------------------\n",
        "# Data (32×32 유지)\n",
        "# ---------------------------\n",
        "if DATASET == 'CIFAR100':\n",
        "    DatasetClass = torchvision.datasets.CIFAR100\n",
        "    NUM_CLASSES = 100\n",
        "else:\n",
        "    DatasetClass = torchvision.datasets.CIFAR10\n",
        "    NUM_CLASSES = 10\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465); std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandAugment(num_ops=1, magnitude=7),  # 가벼운 RandAugment\n",
        "    T.ToTensor(),\n",
        "    Cutout(size=16),                        # 컷아웃 1개\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tf = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = DatasetClass(root='./data', train=True,  download=True, transform=train_tf)\n",
        "testset  = DatasetClass(root='./data', train=False, download=True,  transform=test_tf)\n",
        "\n",
        "# train/val split (9:1)\n",
        "idx = np.arange(len(trainset)); np.random.shuffle(idx)\n",
        "split = int(0.9 * len(idx))\n",
        "train_idx, val_idx = idx[:split], idx[split:]\n",
        "train_subset = torch.utils.data.Subset(trainset, train_idx)\n",
        "val_subset   = torch.utils.data.Subset(trainset, val_idx)\n",
        "\n",
        "dl_kwargs = dict(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=PERSISTENT)\n",
        "if NUM_WORKERS and NUM_WORKERS > 0:\n",
        "    dl_kwargs.update(prefetch_factor=2)\n",
        "\n",
        "train_loader = DataLoader(train_subset, shuffle=True,  **dl_kwargs)\n",
        "val_loader   = DataLoader(val_subset,   shuffle=False, **dl_kwargs)\n",
        "test_loader  = DataLoader(testset,      shuffle=False, **dl_kwargs)\n",
        "\n",
        "# ---------------------------\n",
        "# CIFAR-stem ResNet18\n",
        "# ---------------------------\n",
        "from torchvision import models\n",
        "def build_cifar_resnet18(num_classes):\n",
        "    m = models.resnet18(weights=None)              # 전이 대신 스크래치(32×32에 최적화)\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m\n",
        "\n",
        "model = build_cifar_resnet18(NUM_CLASSES).to(DEVICE)\n",
        "if DEVICE=='cuda':\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "# ---------------------------\n",
        "# Optim / Loss / Schedule / AMP\n",
        "# ---------------------------\n",
        "opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "scaler = GradScaler(enabled=(DEVICE=='cuda'))\n",
        "\n",
        "total_steps = EPOCHS * len(train_loader)\n",
        "warmup_steps = max(1, WARMUP_EPOCHS * len(train_loader))\n",
        "scheduler = get_cosine_schedule_with_warmup(opt, warmup_steps, total_steps)\n",
        "\n",
        "# ---------------------------\n",
        "# Train / Eval\n",
        "# ---------------------------\n",
        "CKPT = '/content/best_cifar_improved.pt'\n",
        "best_val = float('inf')\n",
        "early = EarlyStopping(patience=PATIENCE)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total_loss = 0.0; total_acc = 0.0; n = 0\n",
        "    for x,y in loader:\n",
        "        if DEVICE=='cuda':\n",
        "            x = x.to(DEVICE, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
        "        else:\n",
        "            x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with amp.autocast(device_type='cuda', enabled=(DEVICE=='cuda')):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
        "\n",
        "    if va_loss < best_val:\n",
        "        best_val = va_loss\n",
        "        save_ckpt(model, CKPT)\n",
        "\n",
        "    early.step(va_loss)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
        "          f\"val_loss={va_loss:.4f} acc={va_acc:.4f} | {dt:.1f}s\")\n",
        "    if early.should_stop:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n",
        "\n",
        "# ---------------------------\n",
        "# Test with best\n",
        "# ---------------------------\n",
        "model.load_state_dict(torch.load(CKPT, map_location=DEVICE))\n",
        "model.eval()\n",
        "te_loss, te_acc = run_epoch(test_loader, train=False)\n",
        "print(f\"Test: loss={te_loss:.4f}, acc={te_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bTt_LP-K5Mt",
        "outputId": "edb1a191-0c64-45f6-94d0-46bfc69339b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1f6874e9-b9d3-ca2e-b467-be622eebe3ba)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1402630630.py:150: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(DEVICE=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/30] train_loss=1.9575 acc=0.3122 | val_loss=1.9386 acc=0.3592 | 45.5s\n",
            "[02/30] train_loss=1.6143 acc=0.4881 | val_loss=1.6730 acc=0.4710 | 34.0s\n",
            "[03/30] train_loss=1.4155 acc=0.5829 | val_loss=1.5171 acc=0.5422 | 34.9s\n",
            "[04/30] train_loss=1.2794 acc=0.6512 | val_loss=1.3868 acc=0.6060 | 34.9s\n",
            "[05/30] train_loss=1.1923 acc=0.6911 | val_loss=1.2517 acc=0.6696 | 33.7s\n",
            "[06/30] train_loss=1.1326 acc=0.7210 | val_loss=1.1693 acc=0.7082 | 34.5s\n",
            "[07/30] train_loss=1.0845 acc=0.7419 | val_loss=1.3546 acc=0.6452 | 34.6s\n",
            "[08/30] train_loss=1.0442 acc=0.7609 | val_loss=1.1871 acc=0.6972 | 34.1s\n",
            "[09/30] train_loss=1.0112 acc=0.7745 | val_loss=1.1876 acc=0.6980 | 35.0s\n",
            "[10/30] train_loss=0.9816 acc=0.7893 | val_loss=1.0644 acc=0.7530 | 34.5s\n",
            "[11/30] train_loss=0.9548 acc=0.8000 | val_loss=1.1532 acc=0.7204 | 33.9s\n",
            "[12/30] train_loss=0.9307 acc=0.8108 | val_loss=1.0000 acc=0.7764 | 34.6s\n",
            "[13/30] train_loss=0.9124 acc=0.8192 | val_loss=1.0041 acc=0.7742 | 33.9s\n",
            "[14/30] train_loss=0.8885 acc=0.8294 | val_loss=0.9690 acc=0.7912 | 34.2s\n",
            "[15/30] train_loss=0.8683 acc=0.8387 | val_loss=0.9675 acc=0.7948 | 34.6s\n",
            "[16/30] train_loss=0.8545 acc=0.8444 | val_loss=0.9487 acc=0.8048 | 33.8s\n",
            "[17/30] train_loss=0.8412 acc=0.8510 | val_loss=0.9576 acc=0.8006 | 34.7s\n",
            "[18/30] train_loss=0.8170 acc=0.8604 | val_loss=0.8946 acc=0.8244 | 35.2s\n",
            "[19/30] train_loss=0.8020 acc=0.8676 | val_loss=0.9372 acc=0.8062 | 33.4s\n",
            "[20/30] train_loss=0.7879 acc=0.8727 | val_loss=0.8754 acc=0.8314 | 34.8s\n",
            "[21/30] train_loss=0.7746 acc=0.8796 | val_loss=0.8581 acc=0.8474 | 34.7s\n",
            "[22/30] train_loss=0.7563 acc=0.8874 | val_loss=0.8417 acc=0.8508 | 33.9s\n",
            "[23/30] train_loss=0.7469 acc=0.8919 | val_loss=0.8266 acc=0.8598 | 34.8s\n",
            "[24/30] train_loss=0.7363 acc=0.8969 | val_loss=0.8148 acc=0.8646 | 34.0s\n",
            "[25/30] train_loss=0.7193 acc=0.9049 | val_loss=0.8070 acc=0.8662 | 33.9s\n",
            "[26/30] train_loss=0.7114 acc=0.9085 | val_loss=0.8154 acc=0.8640 | 34.8s\n",
            "[27/30] train_loss=0.7123 acc=0.9079 | val_loss=0.7986 acc=0.8760 | 33.9s\n",
            "[28/30] train_loss=0.7031 acc=0.9133 | val_loss=0.7799 acc=0.8792 | 34.3s\n",
            "[29/30] train_loss=0.6988 acc=0.9141 | val_loss=0.7897 acc=0.8764 | 34.4s\n",
            "[30/30] train_loss=0.6987 acc=0.9127 | val_loss=0.7891 acc=0.8750 | 33.3s\n",
            "Test: loss=0.6794, acc=0.9240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wI1R4eZGLCSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}