{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1FYj_-mkMP5GDOXUse5lpA2CjJQCtKOsr",
      "authorship_tag": "ABX9TyMyD4qmQD+UTGJBWHpiQVal",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heewonLEE2/Data-Ai-Colab/blob/main/Mnist_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A-1. 기본 임포트\n",
        "import os, random, json, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qyzcsoaH6kAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A-2. 재현성(Seed) 고정\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "lCvMHm6i9WJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A-3. 디바이스\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "epnn9LQ-941G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B-1. 변환(정규화 포함) & 약한 증강(회전)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),    # 약한 데이터 증강 (과적합 억제)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "ewYkfj2H96s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B-2. MNIST 다운로드\n",
        "root = './data'\n",
        "train_full = datasets.MNIST(root=root, train=True, download=True, transform=transform_train)\n",
        "test_ds    = datasets.MNIST(root=root, train=False, download=True, transform=transform_eval)"
      ],
      "metadata": {
        "id": "5nLf8lk8-RHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B-3. Train/Val 분리 (90:10)\n",
        "val_ratio = 0.1\n",
        "val_len = int(len(train_full) * val_ratio)\n",
        "train_len = len(train_full) - val_len\n",
        "train_ds, val_ds = random_split(train_full, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n",
        "# 검증셋에는 증강 제거\n",
        "val_ds.dataset.transform = transform_eval"
      ],
      "metadata": {
        "id": "sG7nS29t-md2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B-4. 로더\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "Nz8VoI1N_iAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds), len(val_ds), len(test_ds)"
      ],
      "metadata": {
        "id": "7jb7M9uCCDVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 28->14\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14->7\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# 재생성/검증\n",
        "model = SimpleCNN().to(device)\n",
        "print(hasattr(model, \"forward\"))  # True가 떠야 정상\n",
        "print(model)  # 구조 출력"
      ],
      "metadata": {
        "id": "_KgnYeyVCEw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)"
      ],
      "metadata": {
        "id": "1mDoyMtuFR9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "def evaluate(loader):\n",
        "    return run_epoch(loader, train=False)"
      ],
      "metadata": {
        "id": "ubLpu422FyLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping 설정\n",
        "best_val_acc = 0.0\n",
        "patience, patience_cnt = 3, 0\n",
        "num_epochs = 12\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "\n",
        "os.makedirs('results', exist_ok=True)"
      ],
      "metadata": {
        "id": "yiYm5csfGlnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    history['train_loss'].append(tr_loss)\n",
        "    history['train_acc'].append(tr_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"[{epoch:02d}] train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {val_loss:.4f} acc {val_acc:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # 베스트 모델 저장\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_cnt = 0\n",
        "        torch.save(model.state_dict(), 'results/best_model.pt')\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "4MTxD5zkGsCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 베스트 가중치 로드\n",
        "model.load_state_dict(torch.load('results/best_model.pt', map_location=device))\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"Test loss {test_loss:.4f}, Test acc {test_acc:.4f}\")\n",
        "\n",
        "# 혼동행렬\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        y_true.extend(y.cpu().numpy().tolist())\n",
        "        y_pred.extend(pred.cpu().numpy().tolist())\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "report = classification_report(y_true, y_pred, digits=4)\n",
        "print(report)\n",
        "\n",
        "# 혼동행렬 플롯 저장\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title('Confusion Matrix (MNIST)')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(10)\n",
        "plt.xticks(tick_marks, tick_marks)\n",
        "plt.yticks(tick_marks, tick_marks)\n",
        "plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix.png', dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# 학습 곡선 저장\n",
        "plt.figure()\n",
        "plt.plot(history['train_loss'], label='train_loss')\n",
        "plt.plot(history['val_loss'], label='val_loss')\n",
        "plt.legend(); plt.title('Loss'); plt.xlabel('epoch')\n",
        "plt.savefig('results/loss_curve.png', dpi=150); plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history['train_acc'], label='train_acc')\n",
        "plt.plot(history['val_acc'], label='val_acc')\n",
        "plt.legend(); plt.title('Accuracy'); plt.xlabel('epoch')\n",
        "plt.savefig('results/acc_curve.png', dpi=150); plt.close()\n",
        "\n",
        "# 오분류 샘플 저장\n",
        "mis_imgs, mis_true, mis_pred = [], [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        mask = (p != y)\n",
        "        if mask.any():\n",
        "            mis_imgs.append(x[mask][:16].cpu())\n",
        "            mis_true += y[mask][:16].cpu().tolist()\n",
        "            mis_pred += p[mask][:16].cpu().tolist()\n",
        "        if len(mis_true) >= 16:\n",
        "            break\n",
        "\n",
        "if mis_imgs:\n",
        "    grid = torch.cat(mis_imgs, dim=0)[:16]\n",
        "    grid = (grid * 0.3081 + 0.1307).clamp(0,1)  # 역정규화\n",
        "    fig, axes = plt.subplots(4,4, figsize=(6,6))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(grid[i][0], cmap='gray')\n",
        "        ax.set_title(f\"T:{mis_true[i]} / P:{mis_pred[i]}\")\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/misclassified.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# 메트릭 JSON 저장\n",
        "metrics = {\n",
        "    'best_val_acc': float(best_val_acc),\n",
        "    'test_acc': float(test_acc),\n",
        "    'test_loss': float(test_loss),\n",
        "    'params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': len(history['train_loss']),\n",
        "    'optimizer': 'Adam',\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 1e-4,\n",
        "    'scheduler': 'StepLR(5, gamma=0.7)'\n",
        "}\n",
        "with open('results/metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "metrics"
      ],
      "metadata": {
        "id": "4k0cKhH5HUYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "exp_row = {\n",
        "    'timestamp': int(time.time()),\n",
        "    'seed': 42,\n",
        "    'epochs': metrics['epochs'],\n",
        "    'batch_size': batch_size,\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 1e-4,\n",
        "    'best_val_acc': metrics['best_val_acc'],\n",
        "    'test_acc': metrics['test_acc'],\n",
        "}\n",
        "csv_path = 'results/experiments.csv'\n",
        "file_exists = os.path.exists(csv_path)\n",
        "with open(csv_path, 'a', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(exp_row.keys()))\n",
        "    if not file_exists:\n",
        "        writer.writeheader()\n",
        "    writer.writerow(exp_row)\n",
        "print(f\"Appended to {csv_path}\")"
      ],
      "metadata": {
        "id": "r9VWwqrJHstr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
